import json
import asyncio
import logging

import aio_pika

from app.constants import RABBITMQ_HOST,QUEUE_NAME,PREFETCH_COUNT
from app.services.scraper.publicaciones_scraper import PublicacionesScraper


class RabbitMQConsumer:
    """
    Clase as√≠ncrona que encapsula la conexi√≥n a RabbitMQ y el procesamiento de mensajes.
    """
    def __init__(self):
        self.host = RABBITMQ_HOST
        self.queue_name = QUEUE_NAME
        self.prefetch_count=PREFETCH_COUNT
        self.connection = None
        self.channel = None
        self.queue = None

    async def connect(self):
        """
        Establece la conexi√≥n y el canal con RabbitMQ.
        """
        try:
            self.connection = await aio_pika.connect_robust(
                host=self.host,
                port=5672
            )
            self.channel = await self.connection.channel()
            
            # Configurar prefetch para limitar la cantidad de mensajes sin confirmar.
            await self.channel.set_qos(prefetch_count=self.prefetch_count)
            
            # Declarar la cola como durable para que sobreviva a reinicios
            self.queue = await self.channel.declare_queue(self.queue_name, durable=True)
            logging.info(" Conexi√≥n a RabbitMQ establecida.")
        except Exception as e:
            logging.info(f"‚ùå Error conectando a RabbitMQ: {e}")
            raise

    async def callback(self, message: aio_pika.IncomingMessage):
        """
        Funci√≥n que se ejecuta cuando se recibe un mensaje de RabbitMQ.
        """
        try:
            async with message.process():
                body = message.body.decode()
                data = json.loads(body)
             
                despa_liti = data.get("despa_liti")
                cod_despacho = data.get("cod_despacho")
                ultima_fecha = data.get("ultima_fecha")
                interval_days = data.get("interval_days")

                # Ejecutar el scraper con el process_id recibido
                logging.info(f"Se inicia Scrapper de {despa_liti} - {cod_despacho}")
                scraper = PublicacionesScraper(
                    despa_liti=despa_liti,
                    cod_despacho=cod_despacho,
                    ultima_fecha=ultima_fecha,
                    interval_days=interval_days
                )
                await scraper.run()


        except Exception as e:
            logging.error(f"‚ùå Error procesando mensaje: {str(e).splitlines()[0]}")
            await message.nack(requeue=False)

    async def start_consuming(self):
        """
        Inicia el proceso de consumo de mensajes de forma as√≠ncrona.
        """
        # Si el canal o la cola no est√°n establecidos, se realiza la conexi√≥n
        if self.channel is None or self.queue is None:
            logging.info("El canal o la cola no est√°n establecidos. Conectando...")
            await self.connect()

        # Configuramos el consumidor pas√°ndole la funci√≥n callback
        await self.queue.consume(self.callback)
        logging.info("üéß Esperando mensajes...")

        # Mantener el proceso en ejecuci√≥n para seguir consumiendo mensajes
        try:
            while True:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            logging.info("üëã Interrupci√≥n manual. Cerrando conexi√≥n...")
            await self.connection.close()